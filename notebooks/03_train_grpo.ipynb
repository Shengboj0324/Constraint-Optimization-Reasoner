{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Reinforcement Learning with GRPO (Advanced)\n",
    "\n",
    "This notebook uses **Group Relative Policy Optimization (GRPO)** to further optimize the model.\n",
    "\n",
    "**Key Idea**: Use formal verifiers as reward functions to encourage:\n",
    "1. ✓ **Format Compliance**: Proper XML structure\n",
    "2. ✓ **Feasibility**: Solutions satisfy constraints\n",
    "3. ✓ **Optimality**: Solutions are optimal\n",
    "\n",
    "**Prerequisites**:\n",
    "- Complete `01_train_sft.ipynb` first (SFT baseline required)\n",
    "- Model should be saved at `../models/constraint-reasoner-v1`\n",
    "\n",
    "**Note**: This is an advanced optimization step. The SFT model from Step 1 is already functional.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import jax\n",
    "\n",
    "# Import from installed package (no sys.path hacks!)\n",
    "from src.data_loader import OptimizationDataset\n",
    "from src.format_utils import format_input\n",
    "from src.rewards import (\n",
    "    format_reward_func,\n",
    "    feasibility_reward_func,\n",
    "    optimality_reward_func,\n",
    "    brevity_reward_func,\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 3: REINFORCEMENT LEARNING (GRPO)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "print()\n",
    "\n",
    "# Import Tunix (may not be available in all environments)\n",
    "try:\n",
    "    import tunix\n",
    "    from tunix.rl import GRPOTrainer, RLConfig\n",
    "    from tunix.config import ModelConfig\n",
    "    print(f\"✓ Tunix version: {tunix.__version__}\")\n",
    "    TUNIX_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️  Tunix not available: {e}\")\n",
    "    print(\"This notebook requires Tunix. Install with: pip install google-tunix[prod]\")\n",
    "    TUNIX_AVAILABLE = False\n",
    "\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Load Training Data\n",
    "\n",
    "For RL, we only need prompts (not targets). The model generates completions and learns from rewards.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "RL_DATASET_SIZE = 500  # Increase to 2000+ for production\n",
    "\n",
    "print(f\"Generating {RL_DATASET_SIZE} training prompts...\")\n",
    "dataset = OptimizationDataset(size=RL_DATASET_SIZE)\n",
    "prompts = [format_input(item['problem']) for item in dataset]\n",
    "print(f\"✓ Generated {len(prompts)} prompts for RL training\")\n",
    "print()\n",
    "\n",
    "# Show a sample prompt\n",
    "print(\"Sample Prompt:\")\n",
    "print(prompts[0][:300] + \"...\")\n",
    "print()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Configure Reward Functions\n",
    "\n",
    "GRPO uses multiple reward functions to guide learning. Each reward function scores model outputs.\n",
    "\n",
    "**Prioritized Reward Functions** (per judge recommendations):\n",
    "1. `format_reward_func` (weight: 1.0): Checks XML structure (all tags present)\n",
    "2. `feasibility_reward_func` (weight: 2.0): Verifies constraint satisfaction\n",
    "3. `optimality_reward_func` (weight: 3.0): Verifies solution optimality\n",
    "4. `brevity_reward_func` (weight: 0.5): Encourages concise outputs\n",
    "\n",
    "The model learns to maximize the weighted sum of these rewards.\n",
    "Total possible reward: 1.0 + 2.0 + 3.0 + 0.5 = 6.5\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Tunix RL expects a list of reward functions with prioritized weights\n",
    "reward_funcs = [\n",
    "    format_reward_func,      # Gate 1: Valid format (weight: 1.0)\n",
    "    feasibility_reward_func, # Gate 2: Feasible solution (weight: 2.0)\n",
    "    optimality_reward_func,  # Gate 3: Optimal solution (weight: 3.0)\n",
    "    brevity_reward_func,     # Bonus: Concise output (weight: 0.5)\n",
    "]\n",
    "\n",
    "# Reward weights: higher weights for more critical objectives\n",
    "reward_weights = [1.0, 2.0, 3.0, 0.5]\n",
    "\n",
    "print(\"✓ Configured 4 prioritized reward functions:\")\n",
    "print(\"  1. Format compliance (weight: 1.0)\")\n",
    "print(\"  2. Feasibility verification (weight: 2.0)\")\n",
    "print(\"  3. Optimality verification (weight: 3.0)\")\n",
    "print(\"  4. Brevity bonus (weight: 0.5)\")\n",
    "print(f\"  Total possible reward: {sum(reward_weights)}\")\n",
    "print()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Configure RL Training\n",
    "\n",
    "Set up GRPO training configuration.\n",
    "\n",
    "**Key Parameters**:\n",
    "- `kl_coeff`: KL divergence penalty (keeps model close to SFT baseline)\n",
    "- `num_generations`: Number of completions per prompt for group comparison\n",
    "- `learning_rate`: Lower than SFT (fine-tuning an already good model)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if TUNIX_AVAILABLE:\n",
    "    # RL training configuration\n",
    "    rl_config = RLConfig(\n",
    "        output_dir=\"../checkpoints/grpo_optimized\",\n",
    "        num_train_epochs=1,  # Increase to 2-3 for production\n",
    "        per_device_train_batch_size=4,  # Adjust based on memory\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "        learning_rate=1e-6,  # Much lower than SFT (fine-tuning)\n",
    "        kl_coeff=0.01,  # Penalty for diverging from SFT model\n",
    "        num_generations=4,  # Generate 4 completions per prompt\n",
    "        max_prompt_length=256,\n",
    "        max_completion_length=1024,\n",
    "    )\n",
    "\n",
    "    # Model configuration (start from SFT checkpoint)\n",
    "    sft_model_path = \"../models/constraint-reasoner-v1\"\n",
    "\n",
    "    if not os.path.exists(sft_model_path):\n",
    "        print(f\"⚠️  SFT model not found at: {sft_model_path}\")\n",
    "        print(\"You must run 01_train_sft.ipynb first!\")\n",
    "        TUNIX_AVAILABLE = False\n",
    "    else:\n",
    "        model_config = ModelConfig(\n",
    "            base_model=sft_model_path,  # Start from SFT checkpoint\n",
    "            dtype=\"bfloat16\",\n",
    "            use_flash_attention=True,\n",
    "            lora_rank=8,\n",
    "            lora_alpha=32\n",
    "        )\n",
    "\n",
    "        print(\"✓ RL configuration complete\")\n",
    "        print(f\"  - Base model: {sft_model_path}\")\n",
    "        print(f\"  - Epochs: {rl_config.num_train_epochs}\")\n",
    "        print(f\"  - Learning rate: {rl_config.learning_rate}\")\n",
    "        print(f\"  - KL coefficient: {rl_config.kl_coeff}\")\n",
    "        print(f\"  - Generations per prompt: {rl_config.num_generations}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping configuration (Tunix not available)\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Train with GRPO\n",
    "\n",
    "Run reinforcement learning. This optimizes the model to maximize reward scores.\n",
    "\n",
    "**Expected Improvements**:\n",
    "- Higher format compliance rate\n",
    "- Better constraint satisfaction\n",
    "- More optimal solutions\n",
    "\n",
    "**Note**: This may take several hours depending on hardware.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if TUNIX_AVAILABLE:\n",
    "    try:\n",
    "        print(\"Starting GRPO training...\")\n",
    "        print(\"This may take several hours depending on hardware.\")\n",
    "        print()\n",
    "\n",
    "        trainer = GRPOTrainer(\n",
    "            model_config=model_config,\n",
    "            rl_config=rl_config,\n",
    "            reward_funcs=reward_funcs,\n",
    "            train_dataset=prompts\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the RL-optimized model\n",
    "        rl_model_path = \"../models/constraint-reasoner-v2-rl\"\n",
    "        os.makedirs(os.path.dirname(rl_model_path), exist_ok=True)\n",
    "        trainer.save_model(rl_model_path)\n",
    "\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"✓ GRPO TRAINING COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Model saved to: {rl_model_path}\")\n",
    "        print()\n",
    "        print(\"Next steps:\")\n",
    "        print(\"  1. Run 02_verify_and_export.ipynb with the new model\")\n",
    "        print(\"  2. Compare performance: SFT (v1) vs GRPO (v2)\")\n",
    "        print(\"  3. Export the best model for Kaggle submission\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ GRPO training failed: {e}\")\n",
    "        print(\"Check the error message above for details.\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"⚠️  Cannot train without Tunix\")\n",
    "    print(\"Install Tunix with: pip install google-tunix[prod]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
