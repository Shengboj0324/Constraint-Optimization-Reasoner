{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Supervised Fine-Tuning (SFT) Baseline\n",
    "\n",
    "This notebook trains `google/gemma-2b` on synthetic constraint optimization problems using Tunix (JAX/Flax).\n",
    "\n",
    "**Goal**: Teach the model to output structured reasoning traces with formal certificates.\n",
    "\n",
    "**Prerequisites**:\n",
    "- Run `00_env_check.ipynb` first to verify environment\n",
    "- Ensure `pip install -e .` was run to install the `src` package\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from typing import Dict, List\n",
    "\n",
    "# Import from installed package (no sys.path hacks!)\n",
    "from src.data_loader import OptimizationDataset\n",
    "from src.format_utils import format_input\n",
    "from src.config import config\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: SUPERVISED FINE-TUNING (SFT)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"JAX Devices: {jax.devices()}\")\n",
    "print(f\"JAX Backend: {jax.default_backend()}\")\n",
    "print()\n",
    "\n",
    "# Import Tunix (may not be available in all environments)\n",
    "try:\n",
    "    import tunix\n",
    "    from tunix.config import TrainerConfig, ModelConfig, OptimizerConfig\n",
    "    from tunix.trainer import SFTTrainer\n",
    "    from tunix.data import Dataset as TunixDataset\n",
    "    print(f\"✓ Tunix version: {tunix.__version__}\")\n",
    "    TUNIX_AVAILABLE = True\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️  Tunix not available: {e}\")\n",
    "    print(\"This notebook requires Tunix. Install with: pip install google-tunix[prod]\")\n",
    "    TUNIX_AVAILABLE = False\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Training Data\n",
    "\n",
    "Generate synthetic knapsack problems with ground-truth reasoning traces.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use config for dataset size (can be overridden)\n",
    "DATASET_SIZE = 500  # Increase to 5000+ for production training\n",
    "\n",
    "print(f\"Generating {DATASET_SIZE} training examples...\")\n",
    "dataset = OptimizationDataset(size=DATASET_SIZE)\n",
    "print(f\"✓ Generated {len(dataset)} examples\")\n",
    "print()\n",
    "\n",
    "# Show a sample\n",
    "sample = dataset[0]\n",
    "print(\"Sample Problem:\")\n",
    "print(sample['problem'][:200] + \"...\")\n",
    "print()\n",
    "print(\"Sample Target (first 300 chars):\")\n",
    "print(sample['target'][:300] + \"...\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Tunix\n",
    "\n",
    "Convert our dataset format to Tunix's expected format (prompt/response pairs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_loader: OptimizationDataset) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Convert OptimizationDataset to Tunix format.\n",
    "\n",
    "    Args:\n",
    "        data_loader: Our custom dataset\n",
    "\n",
    "    Returns:\n",
    "        List of dicts with 'prompt' and 'response' keys\n",
    "    \"\"\"\n",
    "    prepared = []\n",
    "    for entry in data_loader:\n",
    "        prepared.append({\n",
    "            \"prompt\": format_input(entry['problem']),\n",
    "            \"response\": entry['target']\n",
    "        })\n",
    "    return prepared\n",
    "\n",
    "if TUNIX_AVAILABLE:\n",
    "    raw_data = prepare_data(dataset)\n",
    "    train_ds = TunixDataset.from_list(raw_data)\n",
    "    print(f\"✓ Prepared Tunix Dataset with {len(train_ds)} items\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping data preparation (Tunix not available)\")\n",
    "\n",
    "print()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Configure Training\n",
    "\n",
    "Set up model, optimizer, and trainer configurations.\n",
    "\n",
    "**Model**: Gemma-2b with LoRA (efficient fine-tuning)\n",
    "**Optimizer**: AdamW with cosine schedule\n",
    "**Training**: 3 epochs with gradient accumulation\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if TUNIX_AVAILABLE:\n",
    "    # Model configuration\n",
    "    model_config = ModelConfig(\n",
    "        base_model=\"google/gemma-2b\",\n",
    "        dtype=\"bfloat16\",  # Memory efficient, TPU optimized\n",
    "        use_flash_attention=True,  # Faster attention computation\n",
    "        lora_rank=8,  # LoRA rank (lower = fewer params)\n",
    "        lora_alpha=32,  # LoRA scaling factor\n",
    "        lora_dropout=0.1  # Regularization\n",
    "    )\n",
    "\n",
    "    # Optimizer configuration\n",
    "    optimizer_config = OptimizerConfig(\n",
    "        learning_rate=2e-5,  # Conservative LR for fine-tuning\n",
    "        scheduler_type=\"cosine\",  # Smooth LR decay\n",
    "        warmup_steps=100,  # Gradual warmup\n",
    "        weight_decay=0.01  # L2 regularization\n",
    "    )\n",
    "\n",
    "    # Trainer configuration\n",
    "    trainer_config = TrainerConfig(\n",
    "        output_dir=\"../checkpoints/sft_baseline\",\n",
    "        num_epochs=3,  # Increase to 5-10 for production\n",
    "        per_device_train_batch_size=4,  # Adjust based on memory\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 16\n",
    "        max_seq_length=1024,  # Max tokens per example\n",
    "        logging_steps=10,  # Log every 10 steps\n",
    "        save_steps=100,  # Checkpoint every 100 steps\n",
    "        eval_steps=50,  # Evaluate every 50 steps (if eval set provided)\n",
    "        save_total_limit=2,  # Keep only 2 latest checkpoints\n",
    "        seed=42  # Reproducibility\n",
    "    )\n",
    "\n",
    "    print(\"✓ Training configuration complete\")\n",
    "    print(f\"  - Base model: {model_config.base_model}\")\n",
    "    print(f\"  - Epochs: {trainer_config.num_epochs}\")\n",
    "    print(f\"  - Effective batch size: {trainer_config.per_device_train_batch_size * trainer_config.gradient_accumulation_steps}\")\n",
    "    print(f\"  - Learning rate: {optimizer_config.learning_rate}\")\n",
    "else:\n",
    "    print(\"⚠️  Skipping configuration (Tunix not available)\")\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Train the Model\n",
    "\n",
    "Run supervised fine-tuning. This will take several hours on TPU, longer on GPU/CPU.\n",
    "\n",
    "**Note**: In Kaggle, ensure you have GPU/TPU enabled in notebook settings.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if TUNIX_AVAILABLE:\n",
    "    try:\n",
    "        print(\"Starting SFT training...\")\n",
    "        print(\"This may take several hours depending on hardware.\")\n",
    "        print()\n",
    "\n",
    "        trainer = SFTTrainer(\n",
    "            model_config=model_config,\n",
    "            trainer_config=trainer_config,\n",
    "            optimizer_config=optimizer_config,\n",
    "            train_dataset=train_ds,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the trained model\n",
    "        model_save_path = \"../models/constraint-reasoner-v1\"\n",
    "        os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "        trainer.save_model(model_save_path)\n",
    "\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"✓ TRAINING COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Model saved to: {model_save_path}\")\n",
    "        print()\n",
    "        print(\"Next steps:\")\n",
    "        print(\"  1. Run 02_verify_and_export.ipynb to test the model\")\n",
    "        print(\"  2. Run 03_train_grpo.ipynb for RL optimization (optional)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Training failed: {e}\")\n",
    "        print(\"Check the error message above for details.\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"⚠️  Cannot train without Tunix\")\n",
    "    print(\"Install Tunix with: pip install google-tunix[prod]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
